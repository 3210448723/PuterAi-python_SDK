#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PuterAI OpenAIå…¼å®¹ä»£ç†æœåŠ¡å™¨

è¿™æ˜¯ä¸€ä¸ªå°†Puter.js AI APIåŒ…è£…ä¸ºOpenAI Python SDKå…¼å®¹æ¥å£çš„ä»£ç†æœåŠ¡å™¨ã€‚
æ”¯æŒèŠå¤©å¯¹è¯ã€å›¾åƒç”Ÿæˆã€è¯­éŸ³åˆæˆã€å›¾åƒç†è§£ç­‰å¤šç§AIåŠŸèƒ½ã€‚

ä¸»è¦åŠŸèƒ½:
- ğŸ¤– èŠå¤©å¯¹è¯ (å…¼å®¹OpenAI Chat Completions API)
- ğŸ–¼ï¸ å›¾åƒç”Ÿæˆ (å…¼å®¹OpenAI DALL-E API)  
- ğŸ”Š æ–‡æœ¬è½¬è¯­éŸ³ (å…¼å®¹OpenAI TTS API)
- ğŸ‘ï¸ å›¾åƒç†è§£ (å…¼å®¹OpenAI Vision API)
- âš¡ æµå¼ä¼ è¾“æ”¯æŒ
- ğŸ”§ å‡½æ•°è°ƒç”¨æ”¯æŒ

è®¸å¯è¯: MIT License
"""

import os
import time
import json
import uuid
import logging
import requests
import base64
from flask import Flask, request, jsonify, Response, stream_with_context
from flask_cors import CORS
from dotenv import load_dotenv
from logging.handlers import RotatingFileHandler

# åŠ è½½ç¯å¢ƒå˜é‡ (æœŸæœ›åœ¨ç¯å¢ƒå˜é‡æˆ–.envæ–‡ä»¶ä¸­è®¾ç½®API_TOKEN)
load_dotenv()

# åˆ›å»ºFlaskåº”ç”¨å®ä¾‹
app = Flask(__name__)
CORS(app)  # å¯ç”¨è·¨åŸŸèµ„æºå…±äº«

# ====== æ—¥å¿—é…ç½®éƒ¨åˆ† ======
def setup_logging():
    """
    é…ç½®åº”ç”¨ç¨‹åºæ—¥å¿—ç³»ç»Ÿ
    
    è®¾ç½®æ–‡ä»¶å’Œæ§åˆ¶å°ä¸¤ç§æ—¥å¿—è¾“å‡ºæ–¹å¼ï¼š
    - æ–‡ä»¶æ—¥å¿—ï¼šå­˜å‚¨åœ¨logs/ç›®å½•ï¼Œæ”¯æŒæ—¥å¿—è½®è½¬
    - æ§åˆ¶å°æ—¥å¿—ï¼šå®æ—¶è¾“å‡ºåˆ°ç»ˆç«¯
    """
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    if not os.path.exists('logs'):
        os.makedirs('logs')
        app.logger.info("åˆ›å»ºæ—¥å¿—ç›®å½•: logs/")

    # é…ç½®æ–‡ä»¶æ—¥å¿—å¤„ç†å™¨ (æ”¯æŒæ—¥å¿—è½®è½¬)
    file_handler = RotatingFileHandler(
        'logs/openai_proxy.log', 
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5  # ä¿ç•™5ä¸ªå¤‡ä»½æ–‡ä»¶
    )
    file_handler.setFormatter(logging.Formatter(
        '%(asctime)s [%(levelname)s] %(message)s (%(pathname)s:%(lineno)d)'
    ))
    file_handler.setLevel(logging.INFO)

    # é…ç½®æ§åˆ¶å°æ—¥å¿—å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(
        '%(asctime)s [%(levelname)s] %(message)s'
    ))
    console_handler.setLevel(logging.INFO)

    # å°†å¤„ç†å™¨æ·»åŠ åˆ°åº”ç”¨æ—¥å¿—å™¨
    app.logger.addHandler(file_handler)
    app.logger.addHandler(console_handler)
    app.logger.setLevel(logging.INFO)

    # é™ä½Flaskå†…ç½®æ—¥å¿—çº§åˆ«ï¼Œå‡å°‘å™ªéŸ³
    logging.getLogger('werkzeug').setLevel(logging.WARNING)
    
    app.logger.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
setup_logging()

# ====== å¸¸é‡é…ç½®éƒ¨åˆ† ======

# Puter APIé…ç½®
PUTER_API_URL = "https://api.puter.com/drivers/call"
PUTER_MODELS_URL = "https://puter.com/puterai/chat/models"

# é»˜è®¤è¯·æ±‚å¤´é…ç½® (æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¯·æ±‚)
PUTER_HEADERS_TEMPLATE = {
    'Accept': '*/*',
    'Content-Type': 'application/json;charset=UTF-8',
    'Origin': 'https://docs.puter.com',
    'Referer': 'https://docs.puter.com/',
    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
}

# é™æ€æ¨¡å‹åˆ—è¡¨ (ä»Puteræ–‡æ¡£ä¸­è·å–çš„éƒ¨åˆ†æ¨¡å‹)
# å®Œæ•´åˆ—è¡¨å‚è§: https://puter.com/puterai/chat/models
PUTER_MODELS_FALLBACK = [
    # OpenAIç³»åˆ—
    "gpt-4o-mini", "gpt-4o", "o1", "o1-mini", "o1-pro", "o3", "o3-mini", "o4-mini",
    "gpt-5", "gpt-5-mini", "gpt-5-nano", "gpt-5-chat-latest", "gpt-4.1", "gpt-4.1-mini",
    "gpt-4.1-nano", "gpt-4.5-preview",
    
    # Anthropic Claudeç³»åˆ—
    "claude-sonnet-4", "claude-opus-4", "claude-3-7-sonnet", "claude-3-5-sonnet",
    
    # å…¶ä»–ä¸»æµæ¨¡å‹
    "deepseek-chat", "deepseek-reasoner", "google/gemini-2.0-flash", "google/gemini-1.5-flash",
    "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo", "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
    "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo", "mistral-large-latest",
    "pixtral-large-latest", "codestral-latest", "google/gemma-2-27b-it", "grok-beta"
]

# OpenAI TTSå£°éŸ³æ˜ å°„åˆ°AWS Pollyå£°éŸ³
TTS_VOICE_MAPPING = {
    "alloy": "Joanna",      # ä¸­æ€§å£°éŸ³
    "echo": "Matthew",      # ç”·æ€§å£°éŸ³
    "fable": "Amy",         # è‹±å¼å¥³æ€§å£°éŸ³
    "onyx": "Brian",        # æ·±æ²‰ç”·æ€§å£°éŸ³
    "nova": "Emma",         # å¹´è½»å¥³æ€§å£°éŸ³
    "shimmer": "Olivia"     # æ¸©æš–å¥³æ€§å£°éŸ³
}

# éŸ³é¢‘æ ¼å¼å¯¹åº”çš„MIMEç±»å‹
AUDIO_CONTENT_TYPE_MAPPING = {
    "mp3": "audio/mpeg",
    "opus": "audio/opus", 
    "aac": "audio/aac",
    "flac": "audio/flac"
}

app.logger.info("å¸¸é‡é…ç½®åŠ è½½å®Œæˆ")

# ====== å·¥å…·å‡½æ•°éƒ¨åˆ† ======

def estimate_tokens(text: str, model: str = "gpt-4o-mini") -> int:
    """
    ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
    
    Args:
        text: è¦ä¼°ç®—çš„æ–‡æœ¬
        model: æ¨¡å‹åç§° (ç”¨äºé€‰æ‹©é€‚å½“çš„ç¼–ç å™¨)
        
    Returns:
        ä¼°ç®—çš„tokenæ•°é‡
        
    Note:
        ä¼˜å…ˆä½¿ç”¨tiktokenåº“è¿›è¡Œç²¾ç¡®ä¼°ç®—ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨ç®€å•çš„å­—ç¬¦æ•°é™¤ä»¥4çš„æ–¹æ³•
    """
    # æ–°å¢ï¼šç¡®ä¿ text ä¸ºå­—ç¬¦ä¸²
    if not isinstance(text, str):
        text = str(text) if text is not None else ""
    try:
        import tiktoken
        try:
            # å°è¯•è·å–æ¨¡å‹å¯¹åº”çš„ç¼–ç å™¨
            enc = tiktoken.encoding_for_model(model)
        except Exception:
            # å›é€€åˆ°é€šç”¨ç¼–ç å™¨
            enc = tiktoken.get_encoding("o200k_base")
        token_count = len(enc.encode(text or ""))
        app.logger.debug(f"ä½¿ç”¨tiktokenä¼°ç®—tokenæ•°é‡: {token_count}")
        return token_count
    except ImportError:
        # tiktokenä¸å¯ç”¨æ—¶çš„å›é€€æ–¹æ¡ˆ: å¤§çº¦1ä¸ªtoken = 4ä¸ªå­—ç¬¦
        token_count = max(1, int(len(text or "") / 4))
        app.logger.debug(f"ä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®—tokenæ•°é‡: {token_count}")
        return token_count


def get_effective_api_key():
    """
    è·å–æœ‰æ•ˆçš„APIå¯†é’¥
    
    ç³»ç»Ÿæ”¯æŒä¸¤ç§APIå¯†é’¥é…ç½®æ–¹å¼ï¼š
    1. è¯·æ±‚å¤´Authorization (ä¼˜å…ˆçº§æ›´é«˜): Bearer your-api-key
    2. ç¯å¢ƒå˜é‡API_TOKEN (å›é€€æ–¹æ¡ˆ)
    
    Returns:
        str: æœ‰æ•ˆçš„APIå¯†é’¥
        
    Note:
        è¯·æ±‚å¤´ä¸­çš„APIå¯†é’¥é•¿åº¦å¿…é¡»å¤§äº8å­—ç¬¦æ‰è¢«è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„
    """
    # æ–¹å¼1: ä»è¯·æ±‚å¤´è·å–APIå¯†é’¥
    auth_header = request.headers.get('Authorization', '')
    if auth_header.startswith('Bearer '):
        request_api_key = auth_header[7:].strip()  # ç§»é™¤ 'Bearer ' å‰ç¼€
        if len(request_api_key) > 8:  # åªæ¥å—é•¿åº¦å¤§äº8çš„å¯†é’¥
            app.logger.debug("ä½¿ç”¨è¯·æ±‚å¤´ä¸­çš„APIå¯†é’¥")
            return request_api_key
        else:
            app.logger.debug("è¯·æ±‚å¤´ä¸­çš„APIå¯†é’¥é•¿åº¦ä¸è¶³ï¼Œå¿½ç•¥")
    
    # æ–¹å¼2: å›é€€åˆ°ç¯å¢ƒå˜é‡
    env_api_key = os.getenv('API_TOKEN', '')
    if env_api_key:
        app.logger.debug("ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„APIå¯†é’¥")
        return env_api_key
    
    app.logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„APIå¯†é’¥")
    return ''


def get_puter_headers(api_key=None):
    """
    ç”ŸæˆPuter APIè¯·æ±‚å¤´
    
    Args:
        api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è·å–
        
    Returns:
        dict: å®Œæ•´çš„è¯·æ±‚å¤´å­—å…¸
    """
    if api_key is None:
        api_key = get_effective_api_key()
    
    headers = PUTER_HEADERS_TEMPLATE.copy()
    headers['Authorization'] = f"Bearer {api_key}"
    
    app.logger.debug("ç”ŸæˆPuter APIè¯·æ±‚å¤´")
    return headers


def extract_usage_from_puter_response(data, model, user_text="", assistant_text=""):
    """
    ä»Puter APIå“åº”ä¸­æå–tokenä½¿ç”¨ä¿¡æ¯
    
    Args:
        data: Puter APIçš„å“åº”æ•°æ®
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        user_text: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ (ç”¨äºæœ¬åœ°ä¼°ç®—)
        assistant_text: åŠ©æ‰‹å›å¤çš„æ–‡æœ¬ (ç”¨äºæœ¬åœ°ä¼°ç®—)
        
    Returns:
        dict: åŒ…å«tokenä½¿ç”¨ç»Ÿè®¡çš„å­—å…¸
    """
    # å°è¯•ä»Puter APIå“åº”ä¸­æå–usageä¿¡æ¯
    result = data.get("result", {})
    puter_usage = result.get("usage", [])
    
    # åˆå§‹åŒ–tokenè®¡æ•°
    prompt_tokens = None
    completion_tokens = None
    
    # è§£æPuterè¿”å›çš„usageæ•°ç»„
    if isinstance(puter_usage, list):
        for usage_item in puter_usage:
            if isinstance(usage_item, dict):
                usage_type = usage_item.get("type")
                amount = usage_item.get("amount")
                
                if usage_type == "prompt" and amount is not None:
                    prompt_tokens = amount
                    app.logger.debug(f"ä»APIè·å–prompt tokens: {amount}")
                elif usage_type == "completion" and amount is not None:
                    completion_tokens = amount
                    app.logger.debug(f"ä»APIè·å–completion tokens: {amount}")
    
    # å¦‚æœAPIæ²¡æœ‰è¿”å›tokenä¿¡æ¯ï¼Œä½¿ç”¨æœ¬åœ°ä¼°ç®—
    if prompt_tokens is None:
        prompt_tokens = estimate_tokens(user_text, model)
        app.logger.debug(f"æœ¬åœ°ä¼°ç®—prompt tokens: {prompt_tokens}")
        
    if completion_tokens is None:
        completion_tokens = estimate_tokens(assistant_text, model)
        app.logger.debug(f"æœ¬åœ°ä¼°ç®—completion tokens: {completion_tokens}")
    
    total_tokens = (prompt_tokens or 0) + (completion_tokens or 0)
    
    usage_info = {
        "prompt_tokens": prompt_tokens,
        "completion_tokens": completion_tokens,
        "total_tokens": total_tokens,
    }
    
    app.logger.info(f"Tokenä½¿ç”¨ç»Ÿè®¡ - æç¤º: {prompt_tokens}, å®Œæˆ: {completion_tokens}, æ€»è®¡: {total_tokens}")
    return usage_info


def normalize_messages(body):
    """
    æ ‡å‡†åŒ–æ¶ˆæ¯æ ¼å¼
    
    å°†ä¸åŒæ ¼å¼çš„è¾“å…¥ç»Ÿä¸€è½¬æ¢ä¸ºæ ‡å‡†çš„OpenAIæ¶ˆæ¯æ ¼å¼ã€‚
    æ”¯æŒçš„è¾“å…¥æ ¼å¼:
    1. æ ‡å‡†çš„messagesæ•°ç»„
    2. ä¼ ç»Ÿçš„promptå­—æ®µ
    3. inputå­—æ®µ
    
    Args:
        body: è¯·æ±‚ä½“å­—å…¸
        
    Returns:
        list: æ ‡å‡†åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨
    """
    messages = body.get("messages")
    
    if not messages:
        # æ”¯æŒä¼ ç»Ÿçš„promptå­—æ®µä½œä¸ºå›é€€
        prompt = body.get("prompt") or body.get("input")
        if isinstance(prompt, str) and prompt:
            messages = [{"role": "user", "content": prompt}]
            app.logger.debug("ä»promptå­—æ®µè½¬æ¢ä¸ºmessagesæ ¼å¼")
    
    # ç¡®ä¿æ¯ä¸ªæ¶ˆæ¯éƒ½æœ‰roleå’Œcontentå­—æ®µ
    normalized = []
    if isinstance(messages, list):
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            # ä¿æŒå›¾åƒå†…å®¹çš„åŸå§‹æ ¼å¼ä¸åšè½¬æ¢ (ç”¨äºVision API)
            normalized.append({"role": role, "content": content})
    
    app.logger.debug(f"æ ‡å‡†åŒ–äº† {len(normalized)} æ¡æ¶ˆæ¯")
    return normalized


def build_openai_chat_response(model: str, assistant_text: str, tool_calls=None, usage=None):
    """
    æ„å»ºOpenAIå…¼å®¹çš„èŠå¤©å“åº”æ ¼å¼
    
    Args:
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        assistant_text: åŠ©æ‰‹å›å¤çš„æ–‡æœ¬
        tool_calls: å·¥å…·è°ƒç”¨ä¿¡æ¯ (å¯é€‰)
        usage: tokenä½¿ç”¨ç»Ÿè®¡ (å¯é€‰)
        
    Returns:
        dict: OpenAIæ ¼å¼çš„å“åº”å­—å…¸
    """
    response_id = f"chatcmpl-{uuid.uuid4().hex[:24]}"
    created = int(time.time())
    
    # æ„å»ºæ¶ˆæ¯å¯¹è±¡
    message = {"role": "assistant", "content": assistant_text}
    if tool_calls:
        message["tool_calls"] = tool_calls
        app.logger.debug(f"æ·»åŠ äº† {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
    
    response = {
        "id": response_id,
        "object": "chat.completion",
        "created": created,
        "model": model,
        "choices": [
            {
                "index": 0,
                "message": message,
                "finish_reason": "stop",
            }
        ],
        "usage": usage or {
            "prompt_tokens": None,
            "completion_tokens": None,
            "total_tokens": None,
        },
        "system_fingerprint": None,
    }
    
    app.logger.debug(f"æ„å»ºOpenAIå“åº”: ID={response_id}, æ¨¡å‹={model}")
    return response


def openai_stream_chunk(data_obj: dict) -> str:
    """
    æ ¼å¼åŒ–OpenAIæµå¼å“åº”æ•°æ®å—
    
    Args:
        data_obj: è¦å‘é€çš„æ•°æ®å¯¹è±¡
        
    Returns:
        str: æ ¼å¼åŒ–åçš„SSEæ•°æ®å—
    """
    return f"data: {json.dumps(data_obj, ensure_ascii=False)}\n\n"


app.logger.info("å·¥å…·å‡½æ•°åˆå§‹åŒ–å®Œæˆ")


# ====== APIç«¯ç‚¹å®ç°éƒ¨åˆ† ======

@app.route("/v1/models", methods=["GET"])
def list_models():
    """
    è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ (å…¼å®¹OpenAI Models API)
    
    é¦–å…ˆå°è¯•ä»Puter APIåŠ¨æ€è·å–æœ€æ–°æ¨¡å‹åˆ—è¡¨ï¼Œ
    å¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å†…ç½®çš„é™æ€æ¨¡å‹åˆ—è¡¨ä½œä¸ºå›é€€ã€‚
    todo: å¯èƒ½åŒ…å«ä¸å¯ç”¨çš„æ¨¡å‹ï¼Œå¦‚ï¼šclaude-3-haiku-20240307ã€arcee_ai/arcee-spotlightã€meta-llama/Meta-Llama-3.1-405B-Instruct-Turboç­‰ï¼Œå»ºè®®é€‰æ‹©çƒ­é—¨æ¨¡å‹ä½¿ç”¨
    
    Returns:
        JSON: OpenAIæ ¼å¼çš„æ¨¡å‹åˆ—è¡¨å“åº”
    """
    app.logger.info("æ”¶åˆ°æ¨¡å‹åˆ—è¡¨è¯·æ±‚")
    data = []
    now = int(time.time())
    
    # éªŒè¯APIå¯†é’¥
    api_key = get_effective_api_key()
    if not api_key:
        app.logger.error("æœªæä¾›æœ‰æ•ˆçš„APIå¯†é’¥")
        return jsonify({
            "error": {
                "message": "æœªæä¾›æœ‰æ•ˆçš„APIå¯†é’¥ã€‚è¯·åœ¨Authorizationå¤´ä¸­æä¾›æˆ–è®¾ç½®API_TOKENç¯å¢ƒå˜é‡",
                "type": "invalid_request_error"
            }
        }), 401
    
    headers = get_puter_headers(api_key)
    
    # å°è¯•ä»Puter APIåŠ¨æ€è·å–æ¨¡å‹åˆ—è¡¨
    try:
        app.logger.debug("æ­£åœ¨ä»Puter APIè·å–æ¨¡å‹åˆ—è¡¨...")
        response = requests.get(PUTER_MODELS_URL, headers=headers, timeout=30)
        if response.status_code == 200:
            models_data = response.json()
            for model in models_data.get("models", []):
                # å¦‚æœæ˜¯å­—å…¸ç±»å‹å¯¹è±¡ï¼Œæ ¸å¯¹æ˜¯å¦ç¬¦åˆopenaiæ¨¡å‹æ ¼å¼
                if isinstance(model, dict):
                    data.append({
                        "id": model["id"] if "id" in model else model.get("name", ""),
                        "object": "model",
                        "created": now,
                        "owned_by": "puter",
                    })
                elif isinstance(model, str):
                    data.append({
                        "id": model,
                        "object": "model",
                        "created": now,
                        "owned_by": "puter",
                    })
            app.logger.info(f"æˆåŠŸä»Puter APIè·å–åˆ° {len(data)} ä¸ªæ¨¡å‹")
            return jsonify({"object": "list", "data": data})
    except Exception as e:
        app.logger.error(f"ä»Puter APIè·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")

    # å›é€€åˆ°é™æ€æ¨¡å‹åˆ—è¡¨
    app.logger.warning("ä½¿ç”¨é™æ€æ¨¡å‹åˆ—è¡¨ä½œä¸ºå›é€€")
    for model_name in PUTER_MODELS_FALLBACK:
        data.append({
            "id": model_name,
            "object": "model",
            "created": now,
            "owned_by": "puter",
        })
    app.logger.info(f"è¿”å› {len(data)} ä¸ªé™æ€æ¨¡å‹")
    return jsonify({"object": "list", "data": data})


@app.route("/v1/chat/completions", methods=["POST"])
def chat_completions():
    """
    èŠå¤©å®ŒæˆAPI (å…¼å®¹OpenAI Chat Completions API)
    
    æ”¯æŒçš„åŠŸèƒ½:
    - ğŸ¤– å¤šæ¨¡å‹èŠå¤©å¯¹è¯
    - ğŸ‘ï¸ å›¾åƒç†è§£ (Vision API)
    - ğŸ”§ å‡½æ•°è°ƒç”¨ (Function Calling)
    - âš¡ æµå¼å“åº”
    - ğŸ›ï¸ å‚æ•°æ§åˆ¶ (temperature, max_tokensç­‰)
    
    Returns:
        JSON/SSE: OpenAIæ ¼å¼çš„èŠå¤©å®Œæˆå“åº”
    """
    app.logger.info("æ”¶åˆ°èŠå¤©å®Œæˆè¯·æ±‚")
    
    # éªŒè¯APIå¯†é’¥
    api_key = get_effective_api_key()
    if not api_key:
        app.logger.error("æœªæä¾›æœ‰æ•ˆçš„APIå¯†é’¥")
        return jsonify({
            "error": {
                "message": "æœªæä¾›æœ‰æ•ˆçš„APIå¯†é’¥ã€‚è¯·åœ¨Authorizationå¤´ä¸­æä¾›æˆ–è®¾ç½®API_TOKENç¯å¢ƒå˜é‡",
                "type": "invalid_request_error"
            }
        }), 401
    
    headers = get_puter_headers(api_key)

    # è§£æè¯·æ±‚å‚æ•°
    body = request.get_json(force=True, silent=True) or {}
    model = body.get("model", "gpt-4.1-nano")
    stream = bool(body.get("stream", False))
    messages = normalize_messages(body)
    temperature = body.get("temperature")
    max_tokens = body.get("max_tokens")
    tools = body.get("tools")  # å‡½æ•°è°ƒç”¨å·¥å…·å®šä¹‰
    
    # æŸäº›æ¨¡å‹ä¸æ”¯æŒtemperatureå‚æ•°ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
    if model in ["o3-mini", "o3", "o4-mini"] and temperature is not None:
        temperature = None  
        app.logger.warning(f"æ¨¡å‹ {model} ä¸æ”¯æŒtemperatureå‚æ•°ï¼Œå·²å¿½ç•¥")

    app.logger.info(f"è¯·æ±‚å‚æ•° - æ¨¡å‹: {model}, æµå¼: {stream}, æ¶ˆæ¯æ•°é‡: {len(messages) if messages else 0}")

    # éªŒè¯å¿…éœ€å‚æ•°
    if not messages:
        app.logger.warning("è¯·æ±‚ä¸­æœªæä¾›æ¶ˆæ¯å†…å®¹")
        return jsonify({"error": {"message": "messageså­—æ®µæ˜¯å¿…éœ€çš„"}}), 400

    # æ£€æµ‹æ˜¯å¦åŒ…å«å›¾åƒå†…å®¹ (Vision APIåŠŸèƒ½)
    has_vision = False
    for message in messages:
        content = message.get("content", "")
        if isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and "image_url" in item:
                    has_vision = True
                    break
        if has_vision:
            break

    # æ„å»ºPuter APIè¯·æ±‚è½½è·
    args = {"messages": messages, "model": model}
    if max_tokens is not None:
        args["max_tokens"] = max_tokens
    if temperature is not None:
        args["temperature"] = temperature
    if tools:
        args["tools"] = tools
        app.logger.debug(f"æ·»åŠ äº† {len(tools)} ä¸ªå·¥å…·å®šä¹‰")
    if has_vision:
        args["vision"] = True
        app.logger.info("å¯ç”¨è§†è§‰æ¨¡å¼ - å¤„ç†å›¾åƒå†…å®¹")
    
    # Puter APIè¯·æ±‚è½½è·ç¤ºä¾‹:
    """
    å‡½æ•°è°ƒç”¨å·¥å…·ç¤ºä¾‹:
    tools = [{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–å¤©æ°”ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                  "location": {
                    "description": "åŸå¸‚åç§°ï¼Œä¾‹å¦‚: åŒ—äº¬, ä¸Šæµ·",
                    "type": "string"
                    }
                  },
                "required": ["location"]
            }
        }
    }]
    """

    payload = {
        "interface": "puter-chat-completion",
        "driver": "openai-completion",
        "method": "complete",
        "args": args,
        "test_mode": False,  # ä¸å¯ç”¨æµ‹è¯•æ¨¡å¼ï¼Œç¡®ä¿è®¡è´¹å’Œtokenä½¿ç”¨
    }
    # å½“modelä¸ºOpenAIå…¼å®¹æ¨¡å‹æ—¶ä½¿ç”¨ï¼ˆæ²¡æœ‰å†’å·ï¼‰ï¼Œå¦‚æœæ˜¯`openrouter:moonshotai/kimi-k2:free`ç­‰å…¶ä»–æ¨¡å‹ï¼Œåˆ™ä½¿ç”¨å¯¹åº”çš„driver=openrouter
    if ":" in model:
        payload["driver"] = model.split(":")[0]  # æå–å†’å·å‰çš„éƒ¨åˆ†ä½œä¸ºdriver

    # Token usage estimation (best-effort)
    try:
        user_text_concat = "\n".join([
            c if isinstance(c, str) else json.dumps(c, ensure_ascii=False)
            for m in messages for c in ([m.get("content")] if not isinstance(m.get("content"), list) else m.get("content"))
        ])
    except Exception:
        user_text_concat = ""

    if stream:
        app.logger.info("Starting streaming response")
        # Attempt true streaming from Puter. If not supported, fall back to single chunk.
        def generate():
            rid = f"chatcmpl-{uuid.uuid4().hex[:24]}"
            created = int(time.time())
            accumulated_content = ""  # è·Ÿè¸ªç´¯ç§¯çš„å“åº”å†…å®¹
            final_usage_data = None  # å­˜å‚¨æœ€ç»ˆçš„usageæ•°æ®

            # Send role chunk first per OpenAI convention
            first_delta = {
                "id": rid,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [
                    {"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}
                ]
            }
            yield openai_stream_chunk(first_delta)

            # Try streaming with Puter
            args_with_stream = dict(args)
            args_with_stream["stream"] = True
            payload_stream = dict(payload)
            payload_stream["args"] = args_with_stream

            try:
                app.logger.debug("Sending streaming request to Puter API")
                with requests.post(PUTER_API_URL, headers=headers, json=payload_stream, stream=True, timeout=30) as r:
                    if r.status_code != 200:
                        app.logger.warning(f"Stream request failed with status {r.status_code}, falling back to non-stream")
                        # Fallback: non-stream request
                        non_stream_resp = requests.post(PUTER_API_URL, headers=headers, json=payload, timeout=120)
                        text_out = ""
                        if non_stream_resp.ok:
                            data_json = non_stream_resp.json()
                            text_out = data_json.get("result", {}).get("message", {}).get("content", "") if data_json.get("success") else non_stream_resp.text
                            # åœ¨fallbackæƒ…å†µä¸‹ä¹Ÿæå–usageä¿¡æ¯
                            if data_json.get("success"):
                                final_usage_data = data_json
                        if text_out:
                            accumulated_content = text_out
                            chunk = {
                                "id": rid,
                                "object": "chat.completion.chunk",
                                "created": created,
                                "model": model,
                                "choices": [
                                    {"index": 0, "delta": {"content": text_out}, "finish_reason": "stop"}
                                ]
                            }
                            yield openai_stream_chunk(chunk)
                        yield "data: [DONE]\n\n"
                        return

                    # Stream line by line; attempt to parse JSON parts with a "text" field
                    for line in r.iter_lines(decode_unicode=True):
                        if not line:
                            continue
                        # Robustly handle bytes or str, e.g.: line = b'{"type":"text","text":""}'
                        if isinstance(line, (bytes, bytearray)):
                            try:
                                enc = getattr(r, "encoding", None) or "utf-8"
                            except Exception:
                                enc = "utf-8"
                            try:
                                s = line.decode(enc, errors="replace").strip()
                            except Exception:
                                s = line.decode("utf-8", errors="replace").strip()
                            app.logger.debug(f"Decoded bytes line using encoding={enc}")
                        else:
                            s = line.strip()
                        
                        # æ·»åŠ è°ƒè¯•æ—¥å¿—
                        app.logger.debug(f"Processing stream line: {repr(s)}")
                        
                        # Some servers send 'data: {...}' lines; normalize
                        if s.startswith("data:"):
                            s = s[5:].strip()
                        
                        # Skip empty lines
                        if not s:
                            continue
                            
                        try:
                            # Try to parse as JSON first
                            part = json.loads(s)
                            # part may be {"type":"text","text":"Hello"} or contain nested structure
                            text_piece = None
                            if isinstance(part, dict):
                                # Puter API æµå¼å“åº”æ ¼å¼: {"type":"text","text":"content"}
                                if part.get("type") == "text" and "text" in part:
                                    text_piece = part.get("text")
                                # ç›´æ¥åŒ…å«textå­—æ®µçš„æ ¼å¼
                                elif "text" in part:
                                    text_piece = part.get("text")
                                # å®Œæ•´å“åº”æ ¼å¼ï¼ˆéæµå¼fallbackæˆ–æœ€ç»ˆchunkï¼‰
                                elif "result" in part and isinstance(part["result"], dict):
                                    text_piece = part["result"].get("message", {}).get("content")
                                    # æ£€æŸ¥æ˜¯å¦åŒ…å«usageä¿¡æ¯
                                    if part.get("result", {}).get("usage"):
                                        final_usage_data = part
                            
                            # Only yield if we have meaningful content
                            if text_piece:
                                accumulated_content += text_piece
                                chunk = {
                                    "id": rid,
                                    "object": "chat.completion.chunk",
                                    "created": created,
                                    "model": model,
                                    "choices": [
                                        {"index": 0, "delta": {"content": text_piece}, "finish_reason": None}
                                    ]
                                }
                                yield openai_stream_chunk(chunk)
                        except json.JSONDecodeError:
                            # If not JSON, just forward as text
                            accumulated_content += s
                            chunk = {
                                "id": rid,
                                "object": "chat.completion.chunk",
                                "created": created,
                                "model": model,
                                "choices": [
                                    {"index": 0, "delta": {"content": s}, "finish_reason": None}
                                ]
                            }
                            yield openai_stream_chunk(chunk)
                        except Exception as e:
                            app.logger.warning(f"Error parsing stream chunk: {e}")
                            continue
            except Exception as e:
                # On error, send as a single final chunk with the error message
                app.logger.error(f"Stream error: {str(e)}")
                accumulated_content = f"[proxy error] {str(e)}"
                err_chunk = {
                    "id": rid,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": model,
                    "choices": [
                        {"index": 0, "delta": {"content": accumulated_content}, "finish_reason": None}
                    ]
                }
                yield openai_stream_chunk(err_chunk)
            finally:
                # è®¡ç®—usageä¿¡æ¯
                if final_usage_data:
                    # ä½¿ç”¨APIè¿”å›çš„usageä¿¡æ¯
                    usage = extract_usage_from_puter_response(final_usage_data, model, user_text_concat, accumulated_content)
                else:
                    # ä½¿ç”¨æœ¬åœ°ä¼°ç®—
                    usage = extract_usage_from_puter_response({}, model, user_text_concat, accumulated_content)
                
                # Send final chunk to indicate completion with usage info
                final_chunk = {
                    "id": rid,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": model,
                    "choices": [
                        {"index": 0, "delta": {}, "finish_reason": "stop"}
                    ],
                    "usage": usage
                }
                yield openai_stream_chunk(final_chunk)
                yield "data: [DONE]\n\n"

        return Response(stream_with_context(generate()), mimetype='text/event-stream')

    # Non-streaming path
    app.logger.info("Processing non-streaming request")
    try:
        app.logger.debug("Sending request to Puter API")
        resp = requests.post(PUTER_API_URL, headers=headers, json=payload, timeout=120)
    except Exception as e:
        app.logger.error(f"Upstream request failed: {str(e)}")
        return jsonify({"error": {"message": f"Upstream error: {str(e)}"}}), 502

    if not resp.ok:
        app.logger.error(f"Upstream returned status {resp.status_code}: {resp.text}")
        return jsonify({"error": {"message": f"Upstream status {resp.status_code}", "details": resp.text}}), 502

    data = resp.json()
    if not data.get("success"):
        app.logger.error(f"Upstream returned error: {data}")
        return jsonify({"error": {"message": "Upstream returned error", "details": data}}), 502

    message_obj = data.get("result", {}).get("message", {})
    assistant_text = message_obj.get("content") or ""
    tool_calls = message_obj.get("tool_calls")

    app.logger.info(f"Response received, length: {len(assistant_text)} chars")

    # ä½¿ç”¨æ–°çš„usageæå–å‡½æ•°ï¼Œä¼˜å…ˆä½¿ç”¨APIè¿”å›çš„tokenä¿¡æ¯
    usage = extract_usage_from_puter_response(data, model, user_text_concat, assistant_text)
    
    app.logger.info(f"éæµå¼å“åº”å®Œæˆ - Tokenä½¿ç”¨: æç¤º={usage['prompt_tokens']}, å®Œæˆ={usage['completion_tokens']}, æ€»è®¡={usage['total_tokens']}")

    return jsonify(build_openai_chat_response(model, assistant_text, tool_calls, usage))


@app.route("/v1/images/generations", methods=["POST"])
def image_generation():
    """
    å›¾åƒç”ŸæˆAPI (å…¼å®¹OpenAI DALL-E API)
    
    é€šè¿‡Puterçš„å›¾åƒç”Ÿæˆæ¥å£åˆ›å»ºå›¾åƒï¼Œæ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ã€‚
    
    æ”¯æŒçš„å‚æ•°:
    - prompt: å›¾åƒæè¿°æ–‡æœ¬ (å¿…éœ€)
    - n: ç”Ÿæˆå›¾åƒæ•°é‡ (é»˜è®¤1)
    - size: å›¾åƒå°ºå¯¸ (é»˜è®¤1024x1024)
    - response_format: è¿”å›æ ¼å¼ (urlæˆ–b64_json)
    
    Returns:
        JSON: OpenAIæ ¼å¼çš„å›¾åƒç”Ÿæˆå“åº”
    """
    app.logger.info("æ”¶åˆ°å›¾åƒç”Ÿæˆè¯·æ±‚")
    
    # éªŒè¯APIå¯†é’¥
    api_key = get_effective_api_key()
    if not api_key:
        app.logger.error("æœªæä¾›æœ‰æ•ˆçš„APIå¯†é’¥")
        return jsonify({
            "error": {
                "message": "æœªæä¾›æœ‰æ•ˆçš„APIå¯†é’¥ã€‚è¯·åœ¨Authorizationå¤´ä¸­æä¾›æˆ–è®¾ç½®API_TOKENç¯å¢ƒå˜é‡",
                "type": "invalid_request_error"
            }
        }), 401
    
    headers = get_puter_headers(api_key)

    # è§£æè¯·æ±‚å‚æ•°
    body = request.get_json(force=True, silent=True) or {}
    prompt = body.get("prompt", "")
    n = body.get("n", 1)  # ç”Ÿæˆå›¾åƒæ•°é‡
    size = body.get("size", "1024x1024")  # å›¾åƒå°ºå¯¸
    response_format = body.get("response_format", "url")  # è¿”å›æ ¼å¼
    
    app.logger.info(f"å›¾åƒç”Ÿæˆå‚æ•° - æç¤ºè¯: '{prompt[:50]}...', æ•°é‡: {n}, å°ºå¯¸: {size}, æ ¼å¼: {response_format}")

    # éªŒè¯å¿…éœ€å‚æ•°
    if not prompt:
        app.logger.warning("å›¾åƒç”Ÿæˆè¯·æ±‚ä¸­æœªæä¾›æç¤ºè¯")
        return jsonify({"error": {"message": "promptå­—æ®µæ˜¯å¿…éœ€çš„"}}), 400

    # æ„å»ºPuter APIè¯·æ±‚è½½è·
    payload = {
        "interface": "puter-image-generation",
        "test_mode": False,  # ä¸å¯ç”¨æµ‹è¯•æ¨¡å¼ï¼Œç¡®ä¿æ­£å¸¸è®¡è´¹
        "method": "generate",
        "args": {
            "prompt": prompt
        }
    }
    
    # æ”¯æŒè‡ªå®šä¹‰å›¾åƒå°ºå¯¸
    if size != "1024x1024":
        try:
            width, height = size.split("x")
            payload["args"]["width"] = int(width)
            payload["args"]["height"] = int(height)
            app.logger.debug(f"è®¾ç½®è‡ªå®šä¹‰å°ºå¯¸: {width}x{height}")
        except (ValueError, IndexError):
            app.logger.warning(f"æ— æ•ˆçš„å°ºå¯¸æ ¼å¼: {size}ï¼Œä½¿ç”¨é»˜è®¤1024x1024")

    try:
        app.logger.debug("å‘Puter APIå‘é€å›¾åƒç”Ÿæˆè¯·æ±‚")
        resp = requests.post(PUTER_API_URL, headers=headers, json=payload, timeout=120)
    except Exception as e:
        app.logger.error(f"å›¾åƒç”Ÿæˆè¯·æ±‚å¤±è´¥: {str(e)}")
        return jsonify({"error": {"message": f"ä¸Šæ¸¸æœåŠ¡é”™è¯¯: {str(e)}"}}), 502

    if not resp.ok:
        app.logger.error(f"å›¾åƒç”Ÿæˆä¸Šæ¸¸æœåŠ¡è¿”å›é”™è¯¯çŠ¶æ€ {resp.status_code}: {resp.text}")
        return jsonify({"error": {"message": f"ä¸Šæ¸¸æœåŠ¡çŠ¶æ€ç  {resp.status_code}", "details": resp.text}}), 502

    # å¤„ç†Puter APIå“åº”
    try:
        if resp.headers.get('content-type', '').startswith('application/json'):
            data = resp.json()
            if not data.get("success"):
                app.logger.error(f"å›¾åƒç”Ÿæˆä¸Šæ¸¸æœåŠ¡è¿”å›é”™è¯¯: {data}")
                return jsonify({"error": {"message": "å›¾åƒç”Ÿæˆå¤±è´¥", "details": data}}), 502
            
            # Puter APIåœ¨resultå­—æ®µä¸­è¿”å›base64å›¾åƒæ•°æ®
            image_data = data.get("result", resp.text)
        else:
            # å¦‚æœç›´æ¥è¿”å›å›¾åƒäºŒè¿›åˆ¶æ•°æ®ï¼Œè½¬æ¢ä¸ºbase64
            image_data = base64.b64encode(resp.content).decode('utf-8')
            app.logger.debug("å°†äºŒè¿›åˆ¶å›¾åƒæ•°æ®è½¬æ¢ä¸ºbase64")
    except Exception as e:
        app.logger.error(f"å¤„ç†å›¾åƒç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": {"message": "å“åº”å¤„ç†é”™è¯¯"}}), 502

    # æ„å»ºOpenAIå…¼å®¹çš„å“åº”æ ¼å¼
    images = []
    for i in range(n):
        if response_format == "b64_json":
            images.append({
                "b64_json": image_data
            })
        else:  # urlæ ¼å¼
            # è¿”å›data URLæ ¼å¼ (åœ¨å®é™…ç”Ÿäº§ä¸­å¯èƒ½éœ€è¦å°†å›¾ç‰‡ä¿å­˜åˆ°æ–‡ä»¶æœåŠ¡å™¨)
            images.append({
                "url": f"data:image/png;base64,{image_data}"
            })

    app.logger.info(f"å›¾åƒç”Ÿæˆå®Œæˆï¼Œè¿”å› {len(images)} å¼ å›¾åƒ")
    return jsonify({
        "created": int(time.time()),
        "data": images
    })



@app.route("/v1/audio/speech", methods=["POST"])
def text_to_speech():
    """
    æ–‡æœ¬è½¬è¯­éŸ³API (å…¼å®¹OpenAI TTS API)
    
    é€šè¿‡Puterçš„AWS Polly TTSæœåŠ¡å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³ã€‚
    æ”¯æŒå¤šç§å£°éŸ³ã€è¯­é€Ÿæ§åˆ¶å’ŒéŸ³é¢‘æ ¼å¼ã€‚
    
    æ”¯æŒçš„å‚æ•°:
    - model: TTSæ¨¡å‹ (tts-1æˆ–tts-1-hd)
    - input: è¦åˆæˆçš„æ–‡æœ¬ (å¿…éœ€)
    - voice: å£°éŸ³ç±»å‹ (alloy, echo, fable, onyx, nova, shimmer)
    - response_format: éŸ³é¢‘æ ¼å¼ (mp3, opus, aac, flac)
    - speed: è¯­é€Ÿ (0.25-4.0ï¼Œé»˜è®¤1.0)
    
    Returns:
        éŸ³é¢‘æ–‡ä»¶çš„äºŒè¿›åˆ¶æ•°æ®
    """
    app.logger.info("æ”¶åˆ°æ–‡æœ¬è½¬è¯­éŸ³è¯·æ±‚")
    
    # éªŒè¯APIå¯†é’¥
    api_key = get_effective_api_key()
    if not api_key:
        app.logger.error("æœªæä¾›æœ‰æ•ˆçš„APIå¯†é’¥")
        return jsonify({
            "error": {
                "message": "æœªæä¾›æœ‰æ•ˆçš„APIå¯†é’¥ã€‚è¯·åœ¨Authorizationå¤´ä¸­æä¾›æˆ–è®¾ç½®API_TOKENç¯å¢ƒå˜é‡",
                "type": "invalid_request_error"
            }
        }), 401
    
    headers = get_puter_headers(api_key)

    # è§£æè¯·æ±‚å‚æ•°
    body = request.get_json(force=True, silent=True) or {}
    model = body.get("model", "tts-1")  # OpenAIæ”¯æŒtts-1å’Œtts-1-hd
    input_text = body.get("input", "")
    voice = body.get("voice", "alloy")  # OpenAIé»˜è®¤å£°éŸ³
    response_format = body.get("response_format", "mp3")  # éŸ³é¢‘æ ¼å¼
    speed = body.get("speed", 1.0)  # è¯­é€Ÿæ§åˆ¶ (0.25-4.0)
    
    app.logger.info(f"TTSå‚æ•° - æ–‡æœ¬: '{input_text[:50]}...', å£°éŸ³: {voice}, æ ¼å¼: {response_format}, è¯­é€Ÿ: {speed}")

    # éªŒè¯å¿…éœ€å‚æ•°
    if not input_text:
        app.logger.warning("TTSè¯·æ±‚ä¸­æœªæä¾›è¾“å…¥æ–‡æœ¬")
        return jsonify({"error": {"message": "inputå­—æ®µæ˜¯å¿…éœ€çš„"}}), 400

    # å°†OpenAIå£°éŸ³æ˜ å°„åˆ°AWS Pollyå£°éŸ³
    puter_voice = TTS_VOICE_MAPPING.get(voice, "Joanna")
    app.logger.debug(f"å£°éŸ³æ˜ å°„: {voice} -> {puter_voice}")
    
    # æ ¹æ®æ¨¡å‹é€‰æ‹©TTSå¼•æ“è´¨é‡
    engine = "neural" if model == "tts-1-hd" else "standard"
    app.logger.debug(f"TTSå¼•æ“: {engine} (åŸºäºæ¨¡å‹: {model})")
    
    # æ„å»ºPuter APIè¯·æ±‚è½½è·
    payload = {
        "interface": "puter-tts",
        "driver": "aws-polly",
        "test_mode": False,  # ä¸å¯ç”¨æµ‹è¯•æ¨¡å¼ï¼Œç¡®ä¿æ­£å¸¸è®¡è´¹
        "method": "synthesize",
        "args": {
            "text": input_text,
            "voice": puter_voice,
            "engine": engine,
            "language": "en-US"  # å¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•å¤šè¯­è¨€æ”¯æŒ
        }
    }
    
    # æ”¯æŒè¯­é€Ÿæ§åˆ¶ (é€šè¿‡SSMLå®ç°)
    if speed != 1.0:
        # AWS Pollyä½¿ç”¨SSMLæ¥æ§åˆ¶è¯­é€Ÿ
        ssml_text = f'<speak><prosody rate="{int(speed * 100)}%">{input_text}</prosody></speak>'
        payload["args"]["text"] = ssml_text
        app.logger.debug(f"åº”ç”¨è¯­é€Ÿæ§åˆ¶: {speed}x -> {int(speed * 100)}%")

    try:
        app.logger.debug("å‘Puter APIå‘é€TTSè¯·æ±‚")
        resp = requests.post(PUTER_API_URL, headers=headers, json=payload, timeout=120)
    except Exception as e:
        app.logger.error(f"TTSè¯·æ±‚å¤±è´¥: {str(e)}")
        return jsonify({"error": {"message": f"ä¸Šæ¸¸æœåŠ¡é”™è¯¯: {str(e)}"}}), 502

    if not resp.ok:
        app.logger.error(f"TTSä¸Šæ¸¸æœåŠ¡è¿”å›é”™è¯¯çŠ¶æ€ {resp.status_code}: {resp.text}")
        return jsonify({"error": {"message": f"ä¸Šæ¸¸æœåŠ¡çŠ¶æ€ç  {resp.status_code}", "details": resp.text}}), 502

    # Puterè¿”å›è¯­éŸ³äºŒè¿›åˆ¶æ•°æ®ï¼Œç›´æ¥è¿”å›ç»™å®¢æˆ·ç«¯
    content_type = AUDIO_CONTENT_TYPE_MAPPING.get(response_format, "audio/mpeg")
    
    app.logger.info(f"TTSåˆæˆå®Œæˆï¼Œç”Ÿæˆ {len(resp.content)} å­—èŠ‚çš„ {response_format} éŸ³é¢‘")
    
    return Response(
        resp.content,
        mimetype=content_type,
        headers={
            "Content-Disposition": f"attachment; filename=speech.{response_format}"
        }
    )


@app.route("/health", methods=["GET"])
def health():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹
    
    ç”¨äºç›‘æ§æœåŠ¡å™¨çŠ¶æ€å’Œå¯ç”¨æ€§ã€‚
    
    Returns:
        JSON: åŒ…å«çŠ¶æ€å’Œæ—¶é—´æˆ³çš„å“åº”
    """
    app.logger.info("æ”¶åˆ°å¥åº·æ£€æŸ¥è¯·æ±‚")
    return jsonify({
        "status": "ok", 
        "timestamp": int(time.time()),
        "version": "1.0.0",
        "service": "PuterAI OpenAI Proxy"
    })


# ====== æœåŠ¡å™¨å¯åŠ¨éƒ¨åˆ† ======

if __name__ == "__main__":
    app.logger.info("="*60)
    app.logger.info("ğŸš€ å¯åŠ¨PuterAI OpenAIå…¼å®¹ä»£ç†æœåŠ¡å™¨")
    app.logger.info("="*60)
    app.logger.info(f"ğŸ“ æœåŠ¡åœ°å€: http://0.0.0.0:9595")
    app.logger.info(f"ğŸ“š APIæ–‡æ¡£: https://platform.openai.com/docs/api-reference")
    app.logger.info(f"ğŸ”‘ APIå¯†é’¥é…ç½®:")
    app.logger.info(f"   æ–¹å¼1: Authorizationå¤´ (æ¨èç”Ÿäº§ç¯å¢ƒ)")
    app.logger.info(f"   æ–¹å¼2: ç¯å¢ƒå˜é‡API_TOKEN (æ¨èå¼€å‘ç¯å¢ƒ)")
    app.logger.info("="*60)
    
    # å¯åŠ¨æœåŠ¡å™¨ (ç¦ç”¨reloaderä»¥é¿å…ä¸debugpyå†²çª)
    app.run(
        host="0.0.0.0", 
        port=9595, 
        debug=True, 
        use_reloader=False
    )
